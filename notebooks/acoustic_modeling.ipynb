{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading scripts\n",
    "sys.path.insert(1, '../scripts')\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\".\")\n",
    "\n",
    "from data_cleaning import DataCleaner\n",
    "from data_viz import Data_Viz\n",
    "\n",
    "DC = DataCleaner(\"../logs/preprocessing_notebook.log\")\n",
    "DV = Data_Viz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set: 16\n",
      "Size of the training set: 4\n"
     ]
    }
   ],
   "source": [
    "# loading meta data\n",
    "\n",
    "train_meta = DC.meta_loader(\"../data/train_meta.json\", \"json\")\n",
    "valid_meta = DC.meta_loader(\"../data/test_meta.json\", \"json\")\n",
    "\n",
    "print(f\"Size of the training set: {len(train_meta)}\")\n",
    "print(f\"Size of the training set: {len(valid_meta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Output</th>\n",
       "      <th>Duration</th>\n",
       "      <th>n_channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ያ ኮምፒ ተር ለ ተጠቃሚው በ ትክክል የሚ ፈለገው ን ነገር እንዲ ያሟላ ...</td>\n",
       "      <td>../data/train/wav/tr_1000_tr11001.wav</td>\n",
       "      <td>../data/train_new/tr_1000_tr11001.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>የ መንገደኞች ማስተናገጃ ህንጻው በ ሰአት እስከ ሶስት ሺ ያህል መንገደኞ...</td>\n",
       "      <td>../data/train/wav/tr_103_tr02003.wav</td>\n",
       "      <td>../data/train_new/tr_103_tr02003.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>በ እ ስቱትጋርት ወደ ትግራይ ያንጋደደ ውን የ ልማት ስራ ኢትዮጵያ ን ን...</td>\n",
       "      <td>../data/train/wav/tr_1006_tr11007.wav</td>\n",
       "      <td>../data/train_new/tr_1006_tr11007.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ግን ወደ ኋላው ላይ ኢሳያስ እንደ ልማ ዳቸው ሁሉን ም የ መልከ ፍ ዲፕሎ...</td>\n",
       "      <td>../data/train/wav/tr_10_tr01010.wav</td>\n",
       "      <td>../data/train_new/tr_10_tr01010.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ፓትርያርኩ ንም እንዳት ቃወ ሟቸው በ ማለት ማስጠንቀቂያ ና ዛቻ አዘል ት...</td>\n",
       "      <td>../data/train/wav/tr_108_tr02008.wav</td>\n",
       "      <td>../data/train_new/tr_108_tr02008.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>መቼም ትላንት ጀምሮ የሰኔ ጾም ገብቷል</td>\n",
       "      <td>../data/train/wav/tr_104_tr02004.wav</td>\n",
       "      <td>../data/train_new/tr_104_tr02004.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>የ ውሀው ዘርፍ ያለበት ን የ ፋይናንስ ችግር ለ መፍታት የ ውሀ ሀብት ል...</td>\n",
       "      <td>../data/train/wav/tr_102_tr02002.wav</td>\n",
       "      <td>../data/train_new/tr_102_tr02002.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>የሚደረግ ባቸውን በደል ና ተንኮል በ ቸልታ ያለፉ መስለው ያደ ባሉ</td>\n",
       "      <td>../data/train/wav/tr_109_tr02009.wav</td>\n",
       "      <td>../data/train_new/tr_109_tr02009.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>አለቃ የጻፏቸው መጽሀፍት ውድ ና ጣፋጭ ከ መሆናቸው የተነሳ በ ህትመታቸው...</td>\n",
       "      <td>../data/train/wav/tr_100_tr01100.wav</td>\n",
       "      <td>../data/train_new/tr_100_tr01100.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>የ ኤርትራውያን ቤት ነዋሪው ን ህብረተሰብ እያ ፋጀ ነው</td>\n",
       "      <td>../data/train/wav/tr_1005_tr11006.wav</td>\n",
       "      <td>../data/train_new/tr_1005_tr11006.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>አፍሪካ ውስጥ ከሚገኙ ት አገሮች ም እንኳ ን ስ ልት ገዛ ቀርቶ እርሷ እ...</td>\n",
       "      <td>../data/train/wav/tr_106_tr02006.wav</td>\n",
       "      <td>../data/train_new/tr_106_tr02006.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>በ ገዳማት የሚገኙ አበው ና መነኮሳት የ እመቤታችን ን ፍቅር ተላብ ሰው ...</td>\n",
       "      <td>../data/train/wav/tr_1004_tr11005.wav</td>\n",
       "      <td>../data/train_new/tr_1004_tr11005.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ሌላው ኢትዮጵያዊ ስ ሊ ሽ ሽ ኔ ስድስተኛ ከ መውጣቱ ጋር ተዳ ም ሮ ኢት...</td>\n",
       "      <td>../data/train/wav/tr_1002_tr11003.wav</td>\n",
       "      <td>../data/train_new/tr_1002_tr11003.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>በ ኮምፒውተር ሳይንስ ፎን ት ቴክኖሎጂ ለ ዶክትሬት ዲግሪ ጥናት እያደረጉ...</td>\n",
       "      <td>../data/train/wav/tr_101_tr02001.wav</td>\n",
       "      <td>../data/train_new/tr_101_tr02001.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>እስራኤል የ ኢትዮጵያ ደጋፊ ና ት በ ማለት ወቀሱ</td>\n",
       "      <td>../data/train/wav/tr_110_tr02010.wav</td>\n",
       "      <td>../data/train_new/tr_110_tr02010.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ሚስት አ ግብተው ሶስት ልጆች በት ነው የ ጠፉት ቄስ ፊሊ ጶ ስ ደብር ተገኙ</td>\n",
       "      <td>../data/train/wav/tr_105_tr02005.wav</td>\n",
       "      <td>../data/train_new/tr_105_tr02005.wav</td>\n",
       "      <td>14.965986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Target  \\\n",
       "0   ያ ኮምፒ ተር ለ ተጠቃሚው በ ትክክል የሚ ፈለገው ን ነገር እንዲ ያሟላ ...   \n",
       "1   የ መንገደኞች ማስተናገጃ ህንጻው በ ሰአት እስከ ሶስት ሺ ያህል መንገደኞ...   \n",
       "2   በ እ ስቱትጋርት ወደ ትግራይ ያንጋደደ ውን የ ልማት ስራ ኢትዮጵያ ን ን...   \n",
       "3   ግን ወደ ኋላው ላይ ኢሳያስ እንደ ልማ ዳቸው ሁሉን ም የ መልከ ፍ ዲፕሎ...   \n",
       "4   ፓትርያርኩ ንም እንዳት ቃወ ሟቸው በ ማለት ማስጠንቀቂያ ና ዛቻ አዘል ት...   \n",
       "5                            መቼም ትላንት ጀምሮ የሰኔ ጾም ገብቷል   \n",
       "6   የ ውሀው ዘርፍ ያለበት ን የ ፋይናንስ ችግር ለ መፍታት የ ውሀ ሀብት ል...   \n",
       "7          የሚደረግ ባቸውን በደል ና ተንኮል በ ቸልታ ያለፉ መስለው ያደ ባሉ   \n",
       "8   አለቃ የጻፏቸው መጽሀፍት ውድ ና ጣፋጭ ከ መሆናቸው የተነሳ በ ህትመታቸው...   \n",
       "9                 የ ኤርትራውያን ቤት ነዋሪው ን ህብረተሰብ እያ ፋጀ ነው   \n",
       "10  አፍሪካ ውስጥ ከሚገኙ ት አገሮች ም እንኳ ን ስ ልት ገዛ ቀርቶ እርሷ እ...   \n",
       "11  በ ገዳማት የሚገኙ አበው ና መነኮሳት የ እመቤታችን ን ፍቅር ተላብ ሰው ...   \n",
       "12  ሌላው ኢትዮጵያዊ ስ ሊ ሽ ሽ ኔ ስድስተኛ ከ መውጣቱ ጋር ተዳ ም ሮ ኢት...   \n",
       "13  በ ኮምፒውተር ሳይንስ ፎን ት ቴክኖሎጂ ለ ዶክትሬት ዲግሪ ጥናት እያደረጉ...   \n",
       "14                    እስራኤል የ ኢትዮጵያ ደጋፊ ና ት በ ማለት ወቀሱ   \n",
       "15   ሚስት አ ግብተው ሶስት ልጆች በት ነው የ ጠፉት ቄስ ፊሊ ጶ ስ ደብር ተገኙ   \n",
       "\n",
       "                                  Feature  \\\n",
       "0   ../data/train/wav/tr_1000_tr11001.wav   \n",
       "1    ../data/train/wav/tr_103_tr02003.wav   \n",
       "2   ../data/train/wav/tr_1006_tr11007.wav   \n",
       "3     ../data/train/wav/tr_10_tr01010.wav   \n",
       "4    ../data/train/wav/tr_108_tr02008.wav   \n",
       "5    ../data/train/wav/tr_104_tr02004.wav   \n",
       "6    ../data/train/wav/tr_102_tr02002.wav   \n",
       "7    ../data/train/wav/tr_109_tr02009.wav   \n",
       "8    ../data/train/wav/tr_100_tr01100.wav   \n",
       "9   ../data/train/wav/tr_1005_tr11006.wav   \n",
       "10   ../data/train/wav/tr_106_tr02006.wav   \n",
       "11  ../data/train/wav/tr_1004_tr11005.wav   \n",
       "12  ../data/train/wav/tr_1002_tr11003.wav   \n",
       "13   ../data/train/wav/tr_101_tr02001.wav   \n",
       "14   ../data/train/wav/tr_110_tr02010.wav   \n",
       "15   ../data/train/wav/tr_105_tr02005.wav   \n",
       "\n",
       "                                   Output   Duration  n_channel  \n",
       "0   ../data/train_new/tr_1000_tr11001.wav  14.965986          2  \n",
       "1    ../data/train_new/tr_103_tr02003.wav  14.965986          2  \n",
       "2   ../data/train_new/tr_1006_tr11007.wav  14.965986          2  \n",
       "3     ../data/train_new/tr_10_tr01010.wav  14.965986          2  \n",
       "4    ../data/train_new/tr_108_tr02008.wav  14.965986          2  \n",
       "5    ../data/train_new/tr_104_tr02004.wav  14.965986          2  \n",
       "6    ../data/train_new/tr_102_tr02002.wav  14.965986          2  \n",
       "7    ../data/train_new/tr_109_tr02009.wav  14.965986          2  \n",
       "8    ../data/train_new/tr_100_tr01100.wav  14.965986          2  \n",
       "9   ../data/train_new/tr_1005_tr11006.wav  14.965986          2  \n",
       "10   ../data/train_new/tr_106_tr02006.wav  14.965986          2  \n",
       "11  ../data/train_new/tr_1004_tr11005.wav  14.965986          2  \n",
       "12  ../data/train_new/tr_1002_tr11003.wav  14.965986          2  \n",
       "13   ../data/train_new/tr_101_tr02001.wav  14.965986          2  \n",
       "14   ../data/train_new/tr_110_tr02010.wav  14.965986          2  \n",
       "15   ../data/train_new/tr_105_tr02005.wav  14.965986          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['', 'ሀ', 'ሁ', 'ሂ', 'ሄ', 'ህ', 'ሆለ', 'ሉ', 'ሊ', 'ላ', 'ሌ', 'ል', 'ሎ', 'ሏመ', 'ሙ', 'ሚ', 'ማ', 'ሜ', 'ም', 'ሞ', 'ሟረ', 'ሩ', 'ሪ', 'ራ', 'ሬ', 'ር', 'ሮ', 'ሯሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷሸ', 'ሹ', 'ሺ', 'ሻ', 'ሼ', 'ሽ', 'ሾ', 'ሿቀ', 'ቁ', 'ቂ', 'ቃ', 'ቄ', 'ቅ', 'ቆ', 'ቋበ', 'ቡ', 'ቢ', 'ባ', 'ቤ', 'ብ', 'ቦ', 'ቧቨ', 'ቩ', 'ቪ', 'ቫ', 'ቬ', 'ቭ', 'ቮ', 'ቯተ', 'ቱ', 'ቲ', 'ታ', 'ቴ', 'ት', 'ቶ', 'ቷቸ', 'ቹ', 'ቺ', 'ቻ', 'ቼ', 'ች', 'ቾ', 'ቿኋነ', 'ኑ', 'ኒ', 'ና', 'ኔ', 'ን', 'ኖ', 'ኗኘ', 'ኙ', 'ኚ', 'ኛ', 'ኜ', 'ኝ', 'ኞ', 'ኟአ', 'ኡ', 'ኢ', 'ኤ', 'እ', 'ኦኧከ', 'ኩ', 'ኪ', 'ካ', 'ኬ', 'ክ', 'ኮኳወ', 'ዉ', 'ዊ', 'ዋ', 'ዌ', 'ው', 'ዎዘ', 'ዙ', 'ዚ', 'ዛ', 'ዜ', 'ዝ', 'ዞ', 'ዟዠ', 'ዡ', 'ዢ', 'ዣ', 'ዤ', 'ዥ', 'ዦ', 'ዧየ', 'ዩ', 'ዪ', 'ያ', 'ዬ', 'ይ', 'ዮደ', 'ዱ', 'ዲ', 'ዳ', 'ዴ', 'ድ', 'ዶ', 'ዷጀ', 'ጁ', 'ጂ', 'ጃ', 'ጄ', 'ጅ', 'ጆ', 'ጇገ', 'ጉ', 'ጊ', 'ጋ', 'ጌ', 'ግ', 'ጐ', 'ጓ', 'ጔጠ', 'ጡ', 'ጢ', 'ጣ', 'ጤ', 'ጥ', 'ጦ', 'ጧጨ', 'ጩ', 'ጪ', 'ጫ', 'ጬ', 'ጭ', 'ጮ', 'ጯጰ', 'ጱ', 'ጲ', 'ጳ', 'ጴ', 'ጵ', 'ጶ', 'ጷፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ', 'ፇፈ', 'ፉ', 'ፊ', 'ፋ', 'ፌ', 'ፍ', 'ፎ', 'ፏፐ', 'ፑ', 'ፒ', 'ፓ', 'ፔ', 'ፕ', 'ፖ'] (size =191)\n"
     ]
    }
   ],
   "source": [
    "# The set of characters accepted in the transcription.\n",
    "amharic_letters = \"\"\"\n",
    "ሀ ሁ ሂ ሄ ህ ሆ\n",
    "ለ ሉ ሊ ላ ሌ ል ሎ ሏ\n",
    "መ ሙ ሚ ማ ሜ ም ሞ ሟ\n",
    "ረ ሩ ሪ ራ ሬ ር ሮ ሯ\n",
    "ሰ ሱ ሲ ሳ ሴ ስ ሶ ሷ\n",
    "ሸ ሹ ሺ ሻ ሼ ሽ ሾ ሿ\n",
    "ቀ ቁ ቂ ቃ ቄ ቅ ቆ ቋ\n",
    "በ ቡ ቢ ባ ቤ ብ ቦ ቧ\n",
    "ቨ ቩ ቪ ቫ ቬ ቭ ቮ ቯ\n",
    "ተ ቱ ቲ ታ ቴ ት ቶ ቷ\n",
    "ቸ ቹ ቺ ቻ ቼ ች ቾ ቿ\n",
    "ኋ\n",
    "ነ ኑ ኒ ና ኔ ን ኖ ኗ\n",
    "ኘ ኙ ኚ ኛ ኜ ኝ ኞ ኟ\n",
    "አ ኡ ኢ ኤ እ ኦ\n",
    "ኧ\n",
    "ከ ኩ ኪ ካ ኬ ክ ኮ\n",
    "ኳ\n",
    "ወ ዉ ዊ ዋ ዌ ው ዎ\n",
    "ዘ ዙ ዚ ዛ ዜ ዝ ዞ ዟ\n",
    "ዠ ዡ ዢ ዣ ዤ ዥ ዦ ዧ\n",
    "የ ዩ ዪ ያ ዬ ይ ዮ\n",
    "ደ ዱ ዲ ዳ ዴ ድ ዶ ዷ\n",
    "ጀ ጁ ጂ ጃ ጄ ጅ ጆ ጇ\n",
    "ገ ጉ ጊ ጋ ጌ ግ ጐ ጓ ጔ\n",
    "ጠ ጡ ጢ ጣ ጤ ጥ ጦ ጧ\n",
    "ጨ ጩ ጪ ጫ ጬ ጭ ጮ ጯ\n",
    "ጰ ጱ ጲ ጳ ጴ ጵ ጶ ጷ\n",
    "ፀ ፁ ፂ ፃ ፄ ፅ ፆ ፇ\n",
    "ፈ ፉ ፊ ፋ ፌ ፍ ፎ ፏ\n",
    "ፐ ፑ ፒ ፓ ፔ ፕ ፖ\n",
    "\"\"\"\n",
    "characters = amharic_letters.replace(\"\\n\", \"\").split(\" \")\n",
    "# Mapping characters to integers\n",
    "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = keras.layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
    "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An integer scalar Tensor. The window length in samples.\n",
    "frame_length = 256\n",
    "# An integer scalar Tensor. The number of samples to step.\n",
    "frame_step = 160\n",
    "# An integer scalar Tensor. The size of the FFT to apply.\n",
    "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
    "fft_length = 384\n",
    "\n",
    "\n",
    "def encode_single_sample(wav_file, label):\n",
    "    ###########################################\n",
    "    ##  Process the Audio\n",
    "    ##########################################\n",
    "    # 1. Read wav file\n",
    "    file = tf.io.read_file(wav_file)\n",
    "    # 2. Decode the wav file\n",
    "    audio, _ = tf.audio.decode_wav(file)\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    # 3. Change type to float\n",
    "    audio = tf.cast(audio, tf.float32)\n",
    "    # 4. Get the spectrogram\n",
    "    spectrogram = tf.signal.stft(\n",
    "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
    "    )\n",
    "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
    "    # 6. normalisation\n",
    "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
    "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
    "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
    "    ###########################################\n",
    "    ##  Process the label\n",
    "    ##########################################\n",
    "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
    "    # 9. Map the characters in label to numbers\n",
    "    label = char_to_num(label)\n",
    "    # 10. Return a dict as our model is expecting two inputs\n",
    "    return spectrogram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "# Define the trainig dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(train_meta[\"Output\"]), list(train_meta[\"Target\"]))\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Define the validation dataset\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(valid_meta[\"Output\"]), list(valid_meta[\"Target\"]))\n",
    ")\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Can not squeeze dim[1], expected a dimension of 1, got 2\n\t [[{{node Squeeze}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\10\\speech_to_text\\notebooks\\acoustic_modeling.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/10/speech_to_text/notebooks/acoustic_modeling.ipynb#ch0000007?line=0'>1</a>\u001b[0m fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/10/speech_to_text/notebooks/acoustic_modeling.ipynb#ch0000007?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_dataset\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/10/speech_to_text/notebooks/acoustic_modeling.ipynb#ch0000007?line=2'>3</a>\u001b[0m     spectrogram \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/10/speech_to_text/notebooks/acoustic_modeling.ipynb#ch0000007?line=3'>4</a>\u001b[0m     spectrogram \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mtrim_zeros(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(spectrogram)])\n",
      "File \u001b[1;32mc:\\Users\\IRONMAN\\miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:761\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=758'>759</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=759'>760</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=760'>761</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[0;32m    <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=761'>762</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=762'>763</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\IRONMAN\\miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:744\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=740'>741</a>\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=741'>742</a>\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=742'>743</a>\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[1;32m--> <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=743'>744</a>\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[0;32m    <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=744'>745</a>\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[0;32m    <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=745'>746</a>\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[0;32m    <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=746'>747</a>\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[0;32m    <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=748'>749</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=749'>750</a>\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/data/ops/iterator_ops.py?line=750'>751</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\IRONMAN\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:2727\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=2724'>2725</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=2725'>2726</a>\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=2726'>2727</a>\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[0;32m   <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=2727'>2728</a>\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[0;32m   <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/ops/gen_dataset_ops.py?line=2728'>2729</a>\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\IRONMAN\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6941\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/framework/ops.py?line=6938'>6939</a>\u001b[0m message \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/framework/ops.py?line=6939'>6940</a>\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/IRONMAN/miniconda3/lib/site-packages/tensorflow/python/framework/ops.py?line=6940'>6941</a>\u001b[0m six\u001b[39m.\u001b[39;49mraise_from(core\u001b[39m.\u001b[39;49m_status_to_exception(e\u001b[39m.\u001b[39;49mcode, message), \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Can not squeeze dim[1], expected a dimension of 1, got 2\n\t [[{{node Squeeze}}]] [Op:IteratorGetNext]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "for batch in train_dataset.take(1):\n",
    "    spectrogram = batch[0][0].numpy()\n",
    "    spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n",
    "    label = batch[1][0]\n",
    "    # Spectrogram\n",
    "    label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.imshow(spectrogram, vmax=1)\n",
    "    ax.set_title(label)\n",
    "    ax.axis(\"off\")\n",
    "    # Wav\n",
    "    file = tf.io.read_file(wavs_path + list(df_train[\"file_name\"])[0] + \".wav\")\n",
    "    audio, _ = tf.audio.decode_wav(file)\n",
    "    audio = audio.numpy()\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    plt.plot(audio)\n",
    "    ax.set_title(\"Signal Wave\")\n",
    "    ax.set_xlim(0, len(audio))\n",
    "    display.display(display.Audio(np.transpose(audio), rate=16000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b997b5b8794c030e25a28be498e8226ee8897df85ede9190db777bdc9cc75be"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
