{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import wave\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '../scripts')\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\".\")\n",
    "\n",
    "from data_cleaning import DataCleaner\n",
    "from data_viz import Data_Viz\n",
    "\n",
    "DC = DataCleaner(\"../logs/preprocessing_notebook.log\")\n",
    "DV = Data_Viz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set: 789\n",
      "Size of the validation set: 197\n"
     ]
    }
   ],
   "source": [
    "train_meta = DC.meta_loader(\"../data/train_meta.csv\", \"csv\")\n",
    "valid_meta = DC.meta_loader(\"../data/test_meta.csv\", \"csv\")\n",
    "\n",
    "print(f\"Size of the training set: {len(train_meta)}\")\n",
    "print(f\"Size of the validation set: {len(valid_meta)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'የተፈራ አተር ቀድሞ ስለ ተዘራ ቶሎ ጐመራ'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta[\"Target\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing | Prepare the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['', 'ሀ', 'ሁ', 'ሂ', 'ሄ', 'ህ', 'ሆ', 'ለ', 'ሉ', 'ሊ', 'ላ', 'ሌ', 'ል', 'ሎ', 'ሏ', 'መ', 'ሙ', 'ሚ', 'ማ', 'ሜ', 'ም', 'ሞ', 'ሟ', 'ረ', 'ሩ', 'ሪ', 'ራ', 'ሬ', 'ር', 'ሮ', 'ሯ', 'ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ', 'ሸ', 'ሹ', 'ሺ', 'ሻ', 'ሼ', 'ሽ', 'ሾ', 'ሿ', 'ቀ', 'ቁ', 'ቂ', 'ቃ', 'ቄ', 'ቅ', 'ቆ', 'ቋ', 'በ', 'ቡ', 'ቢ', 'ባ', 'ቤ', 'ብ', 'ቦ', 'ቧ', 'ቨ', 'ቩ', 'ቪ', 'ቫ', 'ቬ', 'ቭ', 'ቮ', 'ቯ', 'ተ', 'ቱ', 'ቲ', 'ታ', 'ቴ', 'ት', 'ቶ', 'ቷ', 'ቸ', 'ቹ', 'ቺ', 'ቻ', 'ቼ', 'ች', 'ቾ', 'ቿ', 'ኋ', 'ነ', 'ኑ', 'ኒ', 'ና', 'ኔ', 'ን', 'ኖ', 'ኗ', 'ኘ', 'ኙ', 'ኚ', 'ኛ', 'ኜ', 'ኝ', 'ኞ', 'ኟ', 'አ', 'ኡ', 'ኢ', 'ኤ', 'እ', 'ኦ', 'ኧ', 'ከ', 'ኩ', 'ኪ', 'ካ', 'ኬ', 'ክ', 'ኮ', 'ኳ', 'ወ', 'ዉ', 'ዊ', 'ዋ', 'ዌ', 'ው', 'ዎ', 'ዘ', 'ዙ', 'ዚ', 'ዛ', 'ዜ', 'ዝ', 'ዞ', 'ዟ', 'ዠ', 'ዡ', 'ዢ', 'ዣ', 'ዤ', 'ዥ', 'ዦ', 'ዧ', 'የ', 'ዩ', 'ዪ', 'ያ', 'ዬ', 'ይ', 'ዮ', 'ደ', 'ዱ', 'ዲ', 'ዳ', 'ዴ', 'ድ', 'ዶ', 'ዷ', 'ጀ', 'ጁ', 'ጂ', 'ጃ', 'ጄ', 'ጅ', 'ጆ', 'ጇ', 'ገ', 'ጉ', 'ጊ', 'ጋ', 'ጌ', 'ግ', 'ጐ', 'ጓ', 'ጔ', 'ጠ', 'ጡ', 'ጢ', 'ጣ', 'ጤ', 'ጥ', 'ጦ', 'ጧ', 'ጨ', 'ጩ', 'ጪ', 'ጫ', 'ጬ', 'ጭ', 'ጮ', 'ጯ', 'ጰ', 'ጱ', 'ጲ', 'ጳ', 'ጴ', 'ጵ', 'ጶ', 'ጷ', 'ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ', 'ፇ', 'ፈ', 'ፉ', 'ፊ', 'ፋ', 'ፌ', 'ፍ', 'ፎ', 'ፏ', 'ፐ', 'ፑ', 'ፒ', 'ፓ', 'ፔ', 'ፕ', 'ፖ'] (size =221)\n"
     ]
    }
   ],
   "source": [
    "# The set of characters accepted in the transcription.\n",
    "characters = \"\"\"\n",
    "ሀ ሁ ሂ ሄ ህ ሆ\n",
    "ለ ሉ ሊ ላ ሌ ል ሎ ሏ\n",
    "መ ሙ ሚ ማ ሜ ም ሞ ሟ\n",
    "ረ ሩ ሪ ራ ሬ ር ሮ ሯ\n",
    "ሰ ሱ ሲ ሳ ሴ ስ ሶ ሷ\n",
    "ሸ ሹ ሺ ሻ ሼ ሽ ሾ ሿ\n",
    "ቀ ቁ ቂ ቃ ቄ ቅ ቆ ቋ\n",
    "በ ቡ ቢ ባ ቤ ብ ቦ ቧ\n",
    "ቨ ቩ ቪ ቫ ቬ ቭ ቮ ቯ\n",
    "ተ ቱ ቲ ታ ቴ ት ቶ ቷ\n",
    "ቸ ቹ ቺ ቻ ቼ ች ቾ ቿ\n",
    "ኋ\n",
    "ነ ኑ ኒ ና ኔ ን ኖ ኗ\n",
    "ኘ ኙ ኚ ኛ ኜ ኝ ኞ ኟ\n",
    "አ ኡ ኢ ኤ እ ኦ\n",
    "ኧ\n",
    "ከ ኩ ኪ ካ ኬ ክ ኮ\n",
    "ኳ\n",
    "ወ ዉ ዊ ዋ ዌ ው ዎ\n",
    "ዘ ዙ ዚ ዛ ዜ ዝ ዞ ዟ\n",
    "ዠ ዡ ዢ ዣ ዤ ዥ ዦ ዧ\n",
    "የ ዩ ዪ ያ ዬ ይ ዮ\n",
    "ደ ዱ ዲ ዳ ዴ ድ ዶ ዷ\n",
    "ጀ ጁ ጂ ጃ ጄ ጅ ጆ ጇ\n",
    "ገ ጉ ጊ ጋ ጌ ግ ጐ ጓ ጔ\n",
    "ጠ ጡ ጢ ጣ ጤ ጥ ጦ ጧ\n",
    "ጨ ጩ ጪ ጫ ጬ ጭ ጮ ጯ\n",
    "ጰ ጱ ጲ ጳ ጴ ጵ ጶ ጷ\n",
    "ፀ ፁ ፂ ፃ ፄ ፅ ፆ ፇ\n",
    "ፈ ፉ ፊ ፋ ፌ ፍ ፎ ፏ\n",
    "ፐ ፑ ፒ ፓ ፔ ፕ ፖ\n",
    "\"\"\".split()\n",
    "# Mapping characters to integers\n",
    "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = keras.layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
    "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The set of characters accepted in the transcription.\n",
    "# supported = \"\"\"\n",
    "# ሀ ሁ ሂ ሄ ህ ሆ\n",
    "# ለ ሉ ሊ ላ ሌ ል ሎ ሏ\n",
    "# መ ሙ ሚ ማ ሜ ም ሞ ሟ\n",
    "# ረ ሩ ሪ ራ ሬ ር ሮ ሯ\n",
    "# ሰ ሱ ሲ ሳ ሴ ስ ሶ ሷ\n",
    "# ሸ ሹ ሺ ሻ ሼ ሽ ሾ ሿ\n",
    "# ቀ ቁ ቂ ቃ ቄ ቅ ቆ ቋ\n",
    "# በ ቡ ቢ ባ ቤ ብ ቦ ቧ\n",
    "# ቨ ቩ ቪ ቫ ቬ ቭ ቮ ቯ\n",
    "# ተ ቱ ቲ ታ ቴ ት ቶ ቷ\n",
    "# ቸ ቹ ቺ ቻ ቼ ች ቾ ቿ\n",
    "# ኋ\n",
    "# ነ ኑ ኒ ና ኔ ን ኖ ኗ\n",
    "# ኘ ኙ ኚ ኛ ኜ ኝ ኞ ኟ\n",
    "# አ ኡ ኢ ኤ እ ኦ\n",
    "# ኧ\n",
    "# ከ ኩ ኪ ካ ኬ ክ ኮ\n",
    "# ኳ\n",
    "# ወ ዉ ዊ ዋ ዌ ው ዎ\n",
    "# ዘ ዙ ዚ ዛ ዜ ዝ ዞ ዟ\n",
    "# ዠ ዡ ዢ ዣ ዤ ዥ ዦ ዧ\n",
    "# የ ዩ ዪ ያ ዬ ይ ዮ\n",
    "# ደ ዱ ዲ ዳ ዴ ድ ዶ ዷ\n",
    "# ጀ ጁ ጂ ጃ ጄ ጅ ጆ ጇ\n",
    "# ገ ጉ ጊ ጋ ጌ ግ ጐ ጓ ጔ\n",
    "# ጠ ጡ ጢ ጣ ጤ ጥ ጦ ጧ\n",
    "# ጨ ጩ ጪ ጫ ጬ ጭ ጮ ጯ\n",
    "# ጰ ጱ ጲ ጳ ጴ ጵ ጶ ጷ\n",
    "# ፀ ፁ ፂ ፃ ፄ ፅ ፆ ፇ\n",
    "# ፈ ፉ ፊ ፋ ፌ ፍ ፎ ፏ\n",
    "# ፐ ፑ ፒ ፓ ፔ ፕ ፖ\n",
    "# \"\"\".split()\n",
    "\n",
    "# char_map = {}\n",
    "# char_map[\"\"] = 0\n",
    "# char_map[\"<SPACE>\"] = 1\n",
    "# index = 2\n",
    "# for c in supported:\n",
    "#     char_map[c] = index\n",
    "#     index += 1\n",
    "# index_map = {v+1: k for k, v in char_map.items()}\n",
    "\n",
    "# print(\n",
    "#    f\"The vocabulary is: {char_map} \"\n",
    "#     f\"(size ={index_map})\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to create function that describes the transformation to apply to each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Target</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Output</th>\n",
       "      <th>Duration</th>\n",
       "      <th>n_channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>የተፈራ አተር ቀድሞ ስለ ተዘራ ቶሎ ጐመራ</td>\n",
       "      <td>../data/train/wav/tr_10707_tr03124.wav</td>\n",
       "      <td>../data/train_new/tr_10707_tr03124.wav</td>\n",
       "      <td>10.496</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>የ ሱዳን አማጺ ዎች ኢትዮጵያ ንና ኤርትራ ን እየሸ መገሉ ነው</td>\n",
       "      <td>../data/train/wav/tr_10548_tr30109.wav</td>\n",
       "      <td>../data/train_new/tr_10548_tr30109.wav</td>\n",
       "      <td>10.240</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>መ ምሩ እንዳስ ተማሩት ከሆነ ህጻናቶች ም ንም ኩነኔ የ ለ ባቸውም</td>\n",
       "      <td>../data/train/wav/tr_10860_tr09134.wav</td>\n",
       "      <td>../data/train_new/tr_10860_tr09134.wav</td>\n",
       "      <td>8.960</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      Target  \\\n",
       "0           0                  የተፈራ አተር ቀድሞ ስለ ተዘራ ቶሎ ጐመራ   \n",
       "1           1     የ ሱዳን አማጺ ዎች ኢትዮጵያ ንና ኤርትራ ን እየሸ መገሉ ነው   \n",
       "2           2  መ ምሩ እንዳስ ተማሩት ከሆነ ህጻናቶች ም ንም ኩነኔ የ ለ ባቸውም   \n",
       "\n",
       "                                  Feature  \\\n",
       "0  ../data/train/wav/tr_10707_tr03124.wav   \n",
       "1  ../data/train/wav/tr_10548_tr30109.wav   \n",
       "2  ../data/train/wav/tr_10860_tr09134.wav   \n",
       "\n",
       "                                   Output  Duration  n_channel  \n",
       "0  ../data/train_new/tr_10707_tr03124.wav    10.496          2  \n",
       "1  ../data/train_new/tr_10548_tr30109.wav    10.240          2  \n",
       "2  ../data/train_new/tr_10860_tr09134.wav     8.960          2  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ../data/train_new/tr_10707_tr03124.wav\n",
       "1       ../data/train_new/tr_10548_tr30109.wav\n",
       "2       ../data/train_new/tr_10860_tr09134.wav\n",
       "3       ../data/train_new/tr_10443_tr23145.wav\n",
       "4      ../data/train_new/tr_10048_tr098010.wav\n",
       "                        ...                   \n",
       "784    ../data/train_new/tr_10305_tr100027.wav\n",
       "785    ../data/train_new/tr_10203_tr099045.wav\n",
       "786    ../data/train_new/tr_10311_tr100033.wav\n",
       "787     ../data/train_new/tr_10547_tr30108.wav\n",
       "788      ../data/train_new/tr_1111_tr12012.wav\n",
       "Name: Output, Length: 789, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta[\"Output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/train_new/tr_10707_tr03124.wav'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_file = train_meta[\"Output\"][0]\n",
    "wav_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An integer scalar Tensor. The window length in samples.\n",
    "frame_length = 256\n",
    "# An integer scalar Tensor. The number of samples to step.\n",
    "frame_step = 160\n",
    "# An integer scalar Tensor. The size of the FFT to apply.\n",
    "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
    "fft_length = 384\n",
    "# data =  wave.open('../data/train_new/tr_10707_tr03124.wav')\n",
    "def encode_single_sample(wav_file, label):\n",
    "    \"\"\"\n",
    "    Process the Audio\n",
    "    \n",
    "    \"\"\"\n",
    "    #read wav file\n",
    "    file = tf.io.read_file(wav_file)\n",
    "    #decode voice file\n",
    "    audio, _ =tf.audio.decode_wav(file)\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    #change type to float32\n",
    "    audio = tf.cast(audio, tf.float32)    \n",
    "    #get the spectrogram\n",
    "    spectrogram = tf.signal.stft(audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length)\n",
    "    #we only need the magnitude of the spectrogram\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
    "    #normalization\n",
    "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
    "    stddevs= tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
    "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
    "    \"\"\"\n",
    "    Process the label\n",
    "    \"\"\"\n",
    "    #convert label to lower case\n",
    "    label = tf.strings.lower(label)\n",
    "    #split label\n",
    "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
    "    # Map the characters in label to numbers\n",
    "    label = char_to_num(label)\n",
    "    #  Return a dict as our model is expecting two inputs\n",
    "    return spectrogram, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Target</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Output</th>\n",
       "      <th>Duration</th>\n",
       "      <th>n_channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>789</td>\n",
       "      <td>አበራሽ የ ሰፈሩ ነገር ቆስቋሽ መሆኗ የ ታወቀ ነው</td>\n",
       "      <td>../data/train/wav/tr_10719_tr03136.wav</td>\n",
       "      <td>../data/train_new/tr_10719_tr03136.wav</td>\n",
       "      <td>9.984</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>790</td>\n",
       "      <td>ተድላ በልጅነ ቱ ቆረ በ</td>\n",
       "      <td>../data/train/wav/tr_10853_tr09127.wav</td>\n",
       "      <td>../data/train_new/tr_10853_tr09127.wav</td>\n",
       "      <td>4.864</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>791</td>\n",
       "      <td>ለ መሆኑ የ ቁንጅና መለኪያ ምንድነው</td>\n",
       "      <td>../data/train/wav/tr_10251_tr099093.wav</td>\n",
       "      <td>../data/train_new/tr_10251_tr099093.wav</td>\n",
       "      <td>6.144</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                            Target  \\\n",
       "0         789  አበራሽ የ ሰፈሩ ነገር ቆስቋሽ መሆኗ የ ታወቀ ነው   \n",
       "1         790                   ተድላ በልጅነ ቱ ቆረ በ   \n",
       "2         791           ለ መሆኑ የ ቁንጅና መለኪያ ምንድነው   \n",
       "\n",
       "                                   Feature  \\\n",
       "0   ../data/train/wav/tr_10719_tr03136.wav   \n",
       "1   ../data/train/wav/tr_10853_tr09127.wav   \n",
       "2  ../data/train/wav/tr_10251_tr099093.wav   \n",
       "\n",
       "                                    Output  Duration  n_channel  \n",
       "0   ../data/train_new/tr_10719_tr03136.wav     9.984          2  \n",
       "1   ../data/train_new/tr_10853_tr09127.wav     4.864          2  \n",
       "2  ../data/train_new/tr_10251_tr099093.wav     6.144          2  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_meta.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Define the trainig dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(train_meta[\"Target\"]), list(train_meta[\"Output\"]))\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Define the validation dataset\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(valid_meta[\"Target\"]), list(valid_meta[\"Output\"]))\n",
    ")\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining CTC loss function\n",
    "def CTCLoss(y_true, y_pred):\n",
    "    #compute the training-time loss value \n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepSpeech_2\"\n",
      "______________________________________________________________________________________________________________\n",
      " Layer (type)                                    Output Shape                                Param #          \n",
      "==============================================================================================================\n",
      " input (InputLayer)                              [(None, None, 193)]                         0                \n",
      "                                                                                                              \n",
      " expand_dim (Reshape)                            (None, None, 193, 1)                        0                \n",
      "                                                                                                              \n",
      " conv_1 (Conv2D)                                 (None, None, 97, 32)                        14464            \n",
      "                                                                                                              \n",
      " conv_1_bn (BatchNormalization)                  (None, None, 97, 32)                        128              \n",
      "                                                                                                              \n",
      " conv_1_relu (ReLU)                              (None, None, 97, 32)                        0                \n",
      "                                                                                                              \n",
      " conv_2 (Conv2D)                                 (None, None, 49, 32)                        236544           \n",
      "                                                                                                              \n",
      " conv_2_bn (BatchNormalization)                  (None, None, 49, 32)                        128              \n",
      "                                                                                                              \n",
      " conv_2_relu (ReLU)                              (None, None, 49, 32)                        0                \n",
      "                                                                                                              \n",
      " reshape_4 (Reshape)                             (None, None, 1568)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_1 (Bidirectional)                 (None, None, 1024)                          6395904          \n",
      "                                                                                                              \n",
      " dropout_20 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_2 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_21 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_3 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_22 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_4 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_23 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_5 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dense_1 (Dense)                                 (None, None, 1024)                          1049600          \n",
      "                                                                                                              \n",
      " dense_1_relu (ReLU)                             (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " dropout_24 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " dense_4 (Dense)                                 (None, None, 222)                           227550           \n",
      "                                                                                                              \n",
      "==============================================================================================================\n",
      "Total params: 26,823,262\n",
      "Trainable params: 26,823,134\n",
      "Non-trainable params: 128\n",
      "______________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define our model\n",
    "def build_model(input_dim, output_dim, rnn_layers = 5, rnn_units =128):\n",
    "    #Model input\n",
    "    input_spectrogram = keras.layers.Input(shape=(None, input_dim), name=\"input\")\n",
    "    #Expand the dimensions to use 2D CNN\n",
    "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
    "    #convolutions layer 1\n",
    "    x = layers.Conv2D(filters=32, \n",
    "                      kernel_size=(11, 41), \n",
    "                      padding=\"same\", \n",
    "                      strides = [2,2],\n",
    "                      activation=\"relu\", \n",
    "                      name=\"conv_1\")(x)\n",
    "    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
    "    x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
    "    #convolution layer 2\n",
    "    x = layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=[11, 21],\n",
    "        strides=[1, 2],\n",
    "        padding=\"same\",\n",
    "        activation = \"relu\",\n",
    "        use_bias=False,\n",
    "        name=\"conv_2\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
    "    x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
    "    # Reshape the resulted volume to feed the RNNs layers\n",
    "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
    "    # RNN layers\n",
    "    for i in range(1, rnn_layers + 1):\n",
    "        recurrent = layers.GRU(\n",
    "            units=rnn_units,\n",
    "            activation=\"tanh\",\n",
    "            recurrent_activation=\"sigmoid\",\n",
    "            use_bias=True,\n",
    "            return_sequences=True,\n",
    "            reset_after=True,\n",
    "            name=f\"gru_{i}\",\n",
    "        )\n",
    "        x = layers.Bidirectional(\n",
    "            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
    "        )(x)\n",
    "        if i < rnn_layers:\n",
    "            x = layers.Dropout(rate=0.5)(x)\n",
    "    # Dense layer\n",
    "    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n",
    "    x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
    "    x = layers.Dropout(rate=0.5)(x)\n",
    "    # Classification layer\n",
    "    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
    "    # Model\n",
    "    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
    "    # Optimizer\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer=opt, loss=CTCLoss)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the model\n",
    "model = build_model(\n",
    "    input_dim=fft_length // 2 + 1,\n",
    "    output_dim=char_to_num.vocabulary_size(),\n",
    "    rnn_units=512,\n",
    ")\n",
    "model.summary(line_length=110)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traingin the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for result in results:\n",
    "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(result)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "# A callback class to output a few transcriptions during training\n",
    "class CallbackEval(keras.callbacks.Callback):\n",
    "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs=None):\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        for batch in self.dataset:\n",
    "            X, y = batch\n",
    "            batch_predictions = model.predict(X)\n",
    "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
    "            predictions.extend(batch_predictions)\n",
    "            for label in y:\n",
    "                label = (\n",
    "                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "                )\n",
    "                targets.append(label)\n",
    "        wer_score = wer(targets, predictions)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Word Error Rate: {wer_score:.4f}\")\n",
    "        print(\"-\" * 100)\n",
    "        for i in np.random.randint(0, len(predictions), 2):\n",
    "            print(f\"Target    : {targets[i]}\")\n",
    "            print(f\"Prediction: {predictions[i]}\")\n",
    "            print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'validation_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\meron\\Desktop\\10academy\\week4\\speech_to_text\\notebooks\\modeling.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/meron/Desktop/10academy/week4/speech_to_text/notebooks/modeling.ipynb#ch0000023?line=3'>4</a>\u001b[0m validation_callback \u001b[39m=\u001b[39m CallbackEval(validation_dataset)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/meron/Desktop/10academy/week4/speech_to_text/notebooks/modeling.ipynb#ch0000023?line=4'>5</a>\u001b[0m \u001b[39m#Train the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/meron/Desktop/10academy/week4/speech_to_text/notebooks/modeling.ipynb#ch0000023?line=5'>6</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/meron/Desktop/10academy/week4/speech_to_text/notebooks/modeling.ipynb#ch0000023?line=6'>7</a>\u001b[0m     train_dataset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/meron/Desktop/10academy/week4/speech_to_text/notebooks/modeling.ipynb#ch0000023?line=7'>8</a>\u001b[0m     validation_dataset\u001b[39m=\u001b[39;49mvalidation_dataset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/meron/Desktop/10academy/week4/speech_to_text/notebooks/modeling.ipynb#ch0000023?line=8'>9</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/meron/Desktop/10academy/week4/speech_to_text/notebooks/modeling.ipynb#ch0000023?line=9'>10</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[validation_callback],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/meron/Desktop/10academy/week4/speech_to_text/notebooks/modeling.ipynb#ch0000023?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\meron\\anaconda3\\envs\\meron_working\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/meron/anaconda3/envs/meron_working/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/meron/anaconda3/envs/meron_working/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/meron/anaconda3/envs/meron_working/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/meron/anaconda3/envs/meron_working/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/meron/anaconda3/envs/meron_working/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\meron\\anaconda3\\envs\\meron_working\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/meron/anaconda3/envs/meron_working/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/meron/anaconda3/envs/meron_working/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/meron/anaconda3/envs/meron_working/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/meron/anaconda3/envs/meron_working/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/meron/anaconda3/envs/meron_working/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'validation_dataset'"
     ]
    }
   ],
   "source": [
    "#defining the number of epochs\n",
    "epochs = 1\n",
    "#callback function to check transcriptions\n",
    "validation_callback = CallbackEval(validation_dataset)\n",
    "#Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[validation_callback],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff9febebe25a2d7ce3624b0c94722103db22685420c5c5946ccbfd99f2f56a76"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('meron_working')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
